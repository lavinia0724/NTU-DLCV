{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ca1f8-c564-4561-9511-2737229df2fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "NUM_ACCUMULATION_STEPS = 1\n",
    "\n",
    "ENABLE_DUMMY_DATASET = False\n",
    "NUM_WORKERS = 8\n",
    "MAX_TOKENS = 600\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 1e-5\n",
    "\n",
    "CROSS_ATTN_Q_DIM = 4096\n",
    "\n",
    "CROSS_ATTN_EMBED_DIM_1 = 512\n",
    "CROSS_ATTN_NUM_HEADS_1 = 8\n",
    "CROSS_ATTN_KV_DIM_1 = 916\n",
    "\n",
    "CROSS_ATTN_EMBED_DIM_2 = 128\n",
    "CROSS_ATTN_NUM_HEADS_2 = 2\n",
    "CROSS_ATTN_KV_DIM_2 = 21\n",
    "\n",
    "SAVE_MODEL_PREFIX = 'lora_crossattn_1e-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677133-b84a-448d-9a0e-c1671198e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "from CodaDatasets import CodaDataset, DummyDataset\n",
    "from CodaFeatureExtractor import CodaFeatureExtractor\n",
    "from CodaLayers import DecoderWithCrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144aa81-a4da-448c-968a-38640baff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "device = 'cuda'\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a7625-1b89-4958-838d-2a3b69143690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'llava-hf/llava-1.5-7b-hf'\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "question_template = 'USER: {}'\n",
    "answer_template = 'ASSISTANT: {}</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb8d6b-2c87-4b76-8bee-155b342545dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PeftModel.from_pretrained(model, 'models/lora_r64_5e-5_ep1')\n",
    "#model.merge_and_unload()\n",
    "model.load_adapter('models/lora_r64_5e-5_ep1')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dedee-184c-4b40-aa66-d0ec37e9bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee86a5-9ac3-423b-8360-3cabfbbe4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = CodaFeatureExtractor(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb27e3-0a4c-4d4b-a091-08c6e5b8e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in extractor.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c18e9-ecad-4bb4-b8e8-d6ebfa67d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.language_model.model.layers)):\n",
    "    model.language_model.model.layers[i] = DecoderWithCrossAttention(\n",
    "        model.language_model.model.layers[i],\n",
    "        CROSS_ATTN_EMBED_DIM_1,\n",
    "        CROSS_ATTN_EMBED_DIM_2,\n",
    "        CROSS_ATTN_NUM_HEADS_1,\n",
    "        CROSS_ATTN_NUM_HEADS_2,\n",
    "        CROSS_ATTN_Q_DIM,\n",
    "        CROSS_ATTN_KV_DIM_1,\n",
    "        CROSS_ATTN_KV_DIM_2\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042c24f-8b92-4dcb-a5c2-0d0591f48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = {\n",
    "    'train': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='train'),\n",
    "    'val': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='val'),\n",
    "    'test': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40873f0b-06cd-493b-b385-d744ed05eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset['val'] = hf_dataset['val'].shuffle(seed=1234).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9346d64-950c-4aae-8f16-9bc574204e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': CodaDataset(hf_dataset['train'], has_answer=True),\n",
    "    'val': CodaDataset(hf_dataset['val'], has_answer=True),\n",
    "    'test': CodaDataset(hf_dataset['test'], has_answer=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1efd6-43e8-4b2c-8d50-3779d30513d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DUMMY_DATASET:\n",
    "    dataset['train'] = DummyDataset(50, has_answer=True)\n",
    "    dataset['val'] = DummyDataset(2, has_answer=True)\n",
    "    dataset['test'] = DummyDataset(2, has_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d48-a0a3-444f-b039-f8a9d85b3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b7374-69b1-4f1a-9c36-eb2c222e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn),\n",
    "    'val': DataLoader(dataset['val'], batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn),\n",
    "    'test': DataLoader(dataset['test'], batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7522813-b960-41ba-ae6c-3d65a69c53cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_lens = []\n",
    "for _, _, _, _, answer in tqdm(dataset['val']):\n",
    "    answer_len = len(processor.tokenizer(answer)['input_ids'])\n",
    "    answer_lens.append(answer_len)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "sns.ecdfplot(answer_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592e3c8-1fba-4584-8b05-5b4fb959a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_question_types = set()\n",
    "all_questions = set()\n",
    "\n",
    "for _, question_types, _, question, _ in tqdm(dataset['val']):\n",
    "    all_question_types.add(question_types)\n",
    "    all_questions.add(question)\n",
    "longest_question = max(all_questions, key=len)\n",
    "\n",
    "print(repr(all_question_types))\n",
    "print(repr(longest_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240898f-4498-4986-ab8a-90a394262d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader['train']))\n",
    "data_id, question_type, image, question, answer = list(zip(*batch))[0]\n",
    "\n",
    "print('data_id:', repr(data_id))\n",
    "print('question_type:', repr(question_type))\n",
    "display(image.resize((200, 200)))\n",
    "print('question:', repr(question))\n",
    "print('answer:', repr(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843c334-a72f-4866-9e48-9f887654ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.tokenizer('ASSISTANT:', add_special_tokens=False))\n",
    "print(processor.tokenizer('ASSISTANT: Hello world.', add_special_tokens=False))\n",
    "print(processor.tokenizer('ASSISTANT: Goodbye world.', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d102eb-7f45-42eb-bcf1-165898d004d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ae808-df5f-4274-b7e5-677902a11cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_module(module, path):\n",
    "    trainable_names = [name for name, param in module.named_parameters() if param.requires_grad]\n",
    "    trainable_params = {k: v for k, v in module.state_dict().items() if k in trainable_names}\n",
    "    torch.save(trainable_params, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44528ea7-c14f-4d3e-884c-5d5983b8c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "steps_per_epoch = len(dataloader['train']) // NUM_ACCUMULATION_STEPS\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "scaler = torch.GradScaler()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'=== epoch {epoch} ===')\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        loss_total = 0.0\n",
    "\n",
    "        pbar = tqdm(dataloader[phase])\n",
    "        for step, (data_ids, question_types, images, questions, answers) in enumerate(pbar):\n",
    "            batch_size = len(data_ids)\n",
    "            \n",
    "            # stress test\n",
    "            if step == 0:\n",
    "                questions = [longest_question for _ in range(batch_size)]\n",
    "                answers = ['answer ' * 1000 for _ in range(batch_size)]\n",
    "\n",
    "            # 1. disable cross attention\n",
    "            for i in range(len(model.language_model.model.layers)):\n",
    "                model.language_model.model.layers[i].enable_cross_attn = False\n",
    "            \n",
    "            questions = [question_template.format(q) for q in questions]\n",
    "            question_inputs = processor(images=images, text=questions, padding=True, return_tensors='pt').to(device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.autocast(device), torch.no_grad():\n",
    "                question_outputs = model(**question_inputs, use_cache=True)\n",
    "\n",
    "            # 2. enable cross attention\n",
    "            with torch.autocast(device), torch.no_grad():\n",
    "                features = extractor.process_images(images)\n",
    "            for i in range(len(model.language_model.model.layers)):\n",
    "                model.language_model.model.layers[i].enable_cross_attn = True\n",
    "                model.language_model.model.layers[i].cross_attn_context_1 = features['patch_tokens']\n",
    "                model.language_model.model.layers[i].cross_attn_context_2 = features['instance_tokens']\n",
    "                model.language_model.model.layers[i].cross_attn_mask_1 = None\n",
    "                model.language_model.model.layers[i].cross_attn_mask_2 = features['instance_attention_mask']\n",
    "            \n",
    "            answers = [answer_template.format(a) for a in answers]\n",
    "            answer_inputs = processor(text=answers, padding=True, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "\n",
    "            # truncate to avoid OOM\n",
    "            answer_inputs['input_ids'] = answer_inputs['input_ids'][:, :MAX_TOKENS]\n",
    "            answer_inputs['attention_mask'] = answer_inputs['attention_mask'][:, :MAX_TOKENS]\n",
    "\n",
    "            answer_embeds = model.get_input_embeddings()(answer_inputs['input_ids'])\n",
    "            attention_mask = torch.cat([question_inputs['attention_mask'], answer_inputs['attention_mask']], dim=1)\n",
    "            ignored = torch.full((batch_size, 1), -100, device=device)\n",
    "            labels = torch.cat([answer_inputs['input_ids'][:, 1:], ignored], dim=1)\n",
    "            labels[:, :4] = -100  # \"ASSISTANT:\" need not be trained\n",
    "\n",
    "            with torch.autocast(device), torch.set_grad_enabled(phase == 'train'):\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                outputs = model(inputs_embeds=answer_embeds, attention_mask=attention_mask, past_key_values=question_outputs['past_key_values'])\n",
    "                \n",
    "                predictions = outputs['logits']\n",
    "                loss = F.cross_entropy(predictions.reshape(-1, predictions.size(-1)), labels.reshape(-1))\n",
    "                loss_before_scaling = loss.item()\n",
    "                loss /= NUM_ACCUMULATION_STEPS\n",
    "\n",
    "            if step != 0:\n",
    "                pbar.set_postfix_str(f'loss={loss_before_scaling:.3f}')\n",
    "                loss_total += loss_before_scaling\n",
    "\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('lr', optimizer.param_groups[0]['lr'], (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.add_scalar('loss', loss_before_scaling, (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.add_scalar('gate', model.language_model.model.layers[-1].cross_attn_layer_1.gate.item(), (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.add_scalar('gate_2', model.language_model.model.layers[-1].cross_attn_layer_2.gate.item(), (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    if (step + 1) % NUM_ACCUMULATION_STEPS == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "    \n",
    "        loss_avg = loss_total / (len(dataloader[phase]) - 1)\n",
    "        print(f'{phase} loss: {loss_avg}')\n",
    "        \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    save_module(model, f'models/{SAVE_MODEL_PREFIX}_ep{epoch}_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a1ac2-f379-4e50-9481-f5118dcb6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipped\n",
    "model.load_state_dict(torch.load('models/test_ep1_model.pt', weights_only=False), strict=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a063b3-c634-49f4-95c9-92b5f10bf3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for data in tqdm(dataloader['test']):\n",
    "    for data_id, question_type, image, question in zip(*data):\n",
    "        # 1. disable cross attention\n",
    "        for i in range(len(model.language_model.model.layers)):\n",
    "            model.language_model.model.layers[i].enable_cross_attn = False\n",
    "        \n",
    "        question = question_template.format(question)\n",
    "        question_inputs = processor(images=image, text=question, return_tensors='pt').to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            outputs = model(**question_inputs, use_cache=True)\n",
    "\n",
    "        # 2. enable cross attention\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            features = extractor.process_images([image])\n",
    "        for i in range(len(model.language_model.model.layers)):\n",
    "            model.language_model.model.layers[i].enable_cross_attn = True\n",
    "            model.language_model.model.layers[i].cross_attn_context_1 = features['patch_tokens']\n",
    "            model.language_model.model.layers[i].cross_attn_context_2 = features['instance_tokens']\n",
    "            model.language_model.model.layers[i].cross_attn_mask_1 = None\n",
    "            model.language_model.model.layers[i].cross_attn_mask_2 = features['instance_attention_mask']\n",
    "\n",
    "        answer = 'ASSISTANT:'\n",
    "        answer_inputs = processor(text=answer, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "        answer_embeds = model.get_input_embeddings()(answer_inputs['input_ids'])\n",
    "        attention_mask = torch.cat([question_inputs['attention_mask'], answer_inputs['attention_mask']], dim=1)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            outputs = model(inputs_embeds=answer_embeds, attention_mask=attention_mask, past_key_values=outputs['past_key_values'], use_cache=True, num_logits_to_keep=1)\n",
    "\n",
    "        # 3. generate\n",
    "        response = []\n",
    "        for _ in range(MAX_TOKENS):\n",
    "            next_token_id = outputs['logits'].argmax(2)\n",
    "            response.append(next_token_id.item())\n",
    "            if next_token_id.item() == processor.tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=device)], dim=1)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.autocast(device), torch.no_grad():\n",
    "                outputs = model(input_ids=next_token_id, attention_mask=attention_mask, past_key_values=outputs['past_key_values'], use_cache=True)\n",
    "\n",
    "        generated_answer = processor.decode(response, skip_special_tokens=True)\n",
    "        predictions[data_id] = generated_answer\n",
    "        print(repr(data_id))\n",
    "        print(repr(generated_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4ef-00bb-410b-a1f0-b5eb83ac573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
