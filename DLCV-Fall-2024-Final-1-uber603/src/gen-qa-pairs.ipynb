{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ca1f8-c564-4561-9511-2737229df2fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "NUM_WORKERS = 8\n",
    "MAX_TOKENS = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677133-b84a-448d-9a0e-c1671198e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "from CodaDatasets import CodaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144aa81-a4da-448c-968a-38640baff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "device = 'cuda'\n",
    "model_id = 'llava-hf/llava-1.5-7b-hf'\n",
    "prompt_template = 'USER: {} ASSISTANT:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c7bad-f6d6-4bd4-a9e2-2c460280e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a7625-1b89-4958-838d-2a3b69143690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2e5de-fd6f-4b50-92bd-ba9ca6db50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter('models/lora_r64_5e-5_ep1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e36c7e-69e4-4810-8d03-4e0cd405df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042c24f-8b92-4dcb-a5c2-0d0591f48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = {\n",
    "    'train': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='train')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9346d64-950c-4aae-8f16-9bc574204e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': CodaDataset(hf_dataset['train'], has_answer=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d48-a0a3-444f-b039-f8a9d85b3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b7374-69b1-4f1a-9c36-eb2c222e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240898f-4498-4986-ab8a-90a394262d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader['train']))\n",
    "data_id, question_type, image, question, answer = list(zip(*batch))[0]\n",
    "\n",
    "print('data_id:', repr(data_id))\n",
    "print('question_type:', repr(question_type))\n",
    "display(image.resize((200, 200)))\n",
    "print('question:', repr(question))\n",
    "print('answer:', repr(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a5888-8f17-4f5b-af01-34eb00ad5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_qa_prompt_template = \"Given a reference text and a predicted text from an autonomous driving AI assistant, perform the following steps:\\n\\n1. Evaluate the semantic similarity and relevance of the predicted text compared to the reference text.\\n2. Identify key details present in the reference text but missing or inaccurately represented in the predicted text.\\n3. Formulate a question-answer pair starting with 'Q:' for the question and 'A:' for the answer, addressing the missing or misrepresented details. This question-answer pair will be used as additional training data for the AI assistant to improve its reasoning and contextual accuracy.\\n\\nReference text:\\n{}\\n\\nPredicted text:\\n{}\"\n",
    "print(gen_qa_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef9e68-9390-4613-8e3f-0eaa3d8b78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa_pairs = []\n",
    "\n",
    "pbar = tqdm(dataloader['train'])\n",
    "for data in pbar:\n",
    "    for data_id, question_type, image, question, answer in zip(*data):\n",
    "        prompt = prompt_template.format(question)\n",
    "        inputs = processor(images=image, text=prompt, return_tensors='pt').to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=MAX_TOKENS, do_sample=False)\n",
    "        \n",
    "        generated_answer = processor.decode(outputs[0], skip_special_tokens=True).split('ASSISTANT: ')[1]\n",
    "        new_prompt = prompt_template.format(gen_qa_prompt_template.format(repr(answer), repr(generated_answer)))\n",
    "        print(repr(new_prompt))\n",
    "        \n",
    "        inputs = processor(text=new_prompt, return_tensors='pt').to(device)\n",
    "        input_embeds = model_pretrained.get_input_embeddings()(inputs['input_ids'])\n",
    "        outputs = model_pretrained.generate(inputs_embeds=input_embeds, attention_mask=inputs['attention_mask'], max_new_tokens=MAX_TOKENS, do_sample=False)\n",
    "        generated_qa = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(generated_qa)\n",
    "        \n",
    "        pattern = r'Q:\\s*(.*?)\\s*A:\\s*(.*?)(?=Q:|\\Z)'\n",
    "        qa_pairs = re.findall(pattern, generated_qa, flags=re.DOTALL)\n",
    "        qa_pairs = [(data_id, q.strip(), a.strip()) for q, a in qa_pairs]\n",
    "        print(qa_pairs)\n",
    "        \n",
    "        all_qa_pairs.extend(qa_pairs)\n",
    "        pbar.set_postfix_str(f'completed={len(all_qa_pairs)}')\n",
    "\n",
    "        with open('qa_pairs.json', 'w') as f:\n",
    "            json.dump(all_qa_pairs, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
