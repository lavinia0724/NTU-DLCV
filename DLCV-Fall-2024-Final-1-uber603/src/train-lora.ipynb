{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ca1f8-c564-4561-9511-2737229df2fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 1\n",
    "VAL_BATCH_SIZE = 1\n",
    "TEST_BATCH_SIZE = 1\n",
    "NUM_ACCUMULATION_STEPS = 1\n",
    "\n",
    "ENABLE_DUMMY_DATASET = False\n",
    "NUM_WORKERS = 8\n",
    "MAX_TOKENS = 1300\n",
    "\n",
    "EPOCHS = 1\n",
    "LR = 5e-5\n",
    "\n",
    "LORA_RANK = 64\n",
    "LORA_ALPHA = 128\n",
    "\n",
    "SAVE_MODEL_PREFIX = 'lora_r64_5e-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677133-b84a-448d-9a0e-c1671198e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from CodaDatasets import CodaDataset, DummyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144aa81-a4da-448c-968a-38640baff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "device = 'cuda'\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a7625-1b89-4958-838d-2a3b69143690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'llava-hf/llava-1.5-7b-hf'\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "prompt_template = 'USER: {} ASSISTANT:'\n",
    "full_template = 'USER: {} ASSISTANT: {}</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc3fde-7a5d-401a-9dfd-5eebcd5ae015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    excluded_keywords = ['vision_tower', 'multi_modal_projector', 'lm_head']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(keyword in name for keyword in excluded_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            lora_module_names.add(name)\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca69cd0-02c7-48e5-867c-889e343347ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01ab03-ebf7-4095-ae68-33d47b6bc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042c24f-8b92-4dcb-a5c2-0d0591f48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = {\n",
    "    'train': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='train'),\n",
    "    'val': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='val'),\n",
    "    'test': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40873f0b-06cd-493b-b385-d744ed05eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset['val'] = hf_dataset['val'].shuffle(seed=1234).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9346d64-950c-4aae-8f16-9bc574204e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': CodaDataset(hf_dataset['train'], has_answer=True),\n",
    "    'val': CodaDataset(hf_dataset['val'], has_answer=True),\n",
    "    'test': CodaDataset(hf_dataset['test'], has_answer=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1efd6-43e8-4b2c-8d50-3779d30513d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_DUMMY_DATASET:\n",
    "    dataset['train'] = DummyDataset(50, has_answer=True)\n",
    "    dataset['val'] = DummyDataset(2, has_answer=True)\n",
    "    dataset['test'] = DummyDataset(2, has_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d48-a0a3-444f-b039-f8a9d85b3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b7374-69b1-4f1a-9c36-eb2c222e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn),\n",
    "    'val': DataLoader(dataset['val'], batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn),\n",
    "    'test': DataLoader(dataset['test'], batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240898f-4498-4986-ab8a-90a394262d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader['train']))\n",
    "data_id, question_type, image, question, answer = list(zip(*batch))[0]\n",
    "\n",
    "print('data_id:', repr(data_id))\n",
    "print('question_type:', repr(question_type))\n",
    "display(image.resize((200, 200)))\n",
    "print('question:', repr(question))\n",
    "print('answer:', repr(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a34096-4b72-4e2b-853a-ae5b0e59976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = processor.tokenizer('USER: question. ASSISTANT:')\n",
    "full_ids = processor.tokenizer('USER: question. ASSISTANT: answer.</s>')\n",
    "colon_idx = len(prompt_ids['input_ids']) - 1\n",
    "\n",
    "print(prompt_ids)\n",
    "print(full_ids)\n",
    "print(colon_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0677c-a6b9-4bd4-b065-2253fc11da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lens = []\n",
    "for _, _, image, question, answer in tqdm(dataset['val']):\n",
    "    full = full_template.format(question, answer)\n",
    "    full_ids = processor(images=image, text=full, return_tensors='pt')['input_ids']\n",
    "    full_lens.append(full_ids.shape[1])\n",
    "    \n",
    "print(max(full_lens))\n",
    "plt.figure(figsize=(2, 2))\n",
    "sns.ecdfplot(full_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b1c41-63b0-4787-b869-241f6afe323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_question_types = set()\n",
    "all_questions = set()\n",
    "\n",
    "for _, question_types, _, question, _ in tqdm(dataset['val']):\n",
    "    all_question_types.add(question_types)\n",
    "    all_questions.add(question)\n",
    "longest_question = max(all_questions, key=len)\n",
    "\n",
    "print(repr(all_question_types))\n",
    "print(repr(longest_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d102eb-7f45-42eb-bcf1-165898d004d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44528ea7-c14f-4d3e-884c-5d5983b8c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "steps_per_epoch = len(dataloader['train']) // NUM_ACCUMULATION_STEPS\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "    max_lr=LR,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "scaler = torch.GradScaler()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'=== epoch {epoch} ===')\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        loss_total = 0.0\n",
    "\n",
    "        pbar = tqdm(dataloader[phase])\n",
    "        for step, (data_ids, question_types, images, questions, answers) in enumerate(pbar):\n",
    "            batch_size = len(data_ids)\n",
    "            \n",
    "            # stress test\n",
    "            if step == 0:\n",
    "                questions = [longest_question for _ in range(batch_size)]\n",
    "                answers = ['answer ' * 1000 for _ in range(batch_size)]\n",
    "\n",
    "            prompts = [prompt_template.format(q) for q in questions]\n",
    "            prompt_inputs = processor(images=images, text=prompts, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "            fulls = [full_template.format(q, a) for q, a in zip(questions, answers)]\n",
    "            full_inputs = processor(images=images, text=fulls, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "            # truncate to avoid OOM\n",
    "            full_inputs['input_ids'] = full_inputs['input_ids'][:, :MAX_TOKENS]\n",
    "            full_inputs['attention_mask'] = full_inputs['attention_mask'][:, :MAX_TOKENS]\n",
    "\n",
    "            ignored = torch.full((batch_size, 1), -100, device=device)\n",
    "            labels = torch.cat([full_inputs['input_ids'][:, 1:], ignored], dim=1)\n",
    "            for b in range(batch_size):\n",
    "                colon_idx = len(prompt_inputs['input_ids'][b]) - 1\n",
    "                labels[b, :colon_idx] = -100\n",
    "\n",
    "            with torch.autocast(device), torch.set_grad_enabled(phase == 'train'):\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                outputs = model(**full_inputs)\n",
    "                \n",
    "                predictions = outputs['logits']\n",
    "                loss = F.cross_entropy(predictions.reshape(-1, predictions.size(-1)), labels.reshape(-1))\n",
    "                loss_before_scaling = loss.item()\n",
    "                loss /= NUM_ACCUMULATION_STEPS\n",
    "\n",
    "            if step != 0:\n",
    "                pbar.set_postfix_str(f'loss={loss_before_scaling:.3f}')\n",
    "                loss_total += loss_before_scaling\n",
    "\n",
    "                if phase == 'train':\n",
    "                    writer.add_scalar('lr', optimizer.param_groups[0]['lr'], (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.add_scalar('loss', loss_before_scaling, (epoch-1)*steps_per_epoch+step)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    if (step + 1) % NUM_ACCUMULATION_STEPS == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "    \n",
    "        loss_avg = loss_total / (len(dataloader[phase]) - 1)\n",
    "        print(f'{phase} loss: {loss_avg}')\n",
    "        \n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    model.save_pretrained(f'models/{SAVE_MODEL_PREFIX}_ep{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef9e68-9390-4613-8e3f-0eaa3d8b78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for data_ids, question_types, images, questions in tqdm(dataloader['test']):\n",
    "    prompts = [prompt_template.format(q) for q in questions]\n",
    "    inputs = processor(images=images, text=prompts, padding=True, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_TOKENS, do_sample=False)\n",
    "    for data_id, output in zip(data_ids, outputs):\n",
    "        generated_answer = processor.decode(output, skip_special_tokens=True).split('ASSISTANT: ')[1]\n",
    "        predictions[data_id] = generated_answer\n",
    "        print(repr(data_id))\n",
    "        print(repr(generated_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4ef-00bb-410b-a1f0-b5eb83ac573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
