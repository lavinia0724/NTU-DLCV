{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ca1f8-c564-4561-9511-2737229df2fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 1\n",
    "NUM_WORKERS = 8\n",
    "MAX_TOKENS = 600\n",
    "\n",
    "CROSS_ATTN_Q_DIM = 4096\n",
    "\n",
    "CROSS_ATTN_EMBED_DIM_1 = 512\n",
    "CROSS_ATTN_NUM_HEADS_1 = 8\n",
    "CROSS_ATTN_KV_DIM_1 = 916\n",
    "\n",
    "CROSS_ATTN_EMBED_DIM_2 = 128\n",
    "CROSS_ATTN_NUM_HEADS_2 = 2\n",
    "CROSS_ATTN_KV_DIM_2 = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677133-b84a-448d-9a0e-c1671198e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "from CodaDatasets import CodaDataset\n",
    "from CodaFeatureExtractor import CodaFeatureExtractor\n",
    "from CodaLayers import DecoderWithCrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144aa81-a4da-448c-968a-38640baff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a7625-1b89-4958-838d-2a3b69143690",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'llava-hf/llava-1.5-7b-hf'\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014691b-ba9e-4e74-ab25-5aba81c04a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = 'USER: {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb8d6b-2c87-4b76-8bee-155b342545dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_adapter('models/lora_r64_5e-5_ep1')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee86a5-9ac3-423b-8360-3cabfbbe4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = CodaFeatureExtractor(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c18e9-ecad-4bb4-b8e8-d6ebfa67d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.language_model.model.layers)):\n",
    "    model.language_model.model.layers[i] = DecoderWithCrossAttention(\n",
    "        model.language_model.model.layers[i],\n",
    "        CROSS_ATTN_EMBED_DIM_1,\n",
    "        CROSS_ATTN_EMBED_DIM_2,\n",
    "        CROSS_ATTN_NUM_HEADS_1,\n",
    "        CROSS_ATTN_NUM_HEADS_2,\n",
    "        CROSS_ATTN_Q_DIM,\n",
    "        CROSS_ATTN_KV_DIM_1,\n",
    "        CROSS_ATTN_KV_DIM_2\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042c24f-8b92-4dcb-a5c2-0d0591f48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = {\n",
    "    'test': datasets.load_dataset('ntudlcv/dlcv_2024_final1', split='test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9346d64-950c-4aae-8f16-9bc574204e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'test': CodaDataset(hf_dataset['test'], has_answer=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d48-a0a3-444f-b039-f8a9d85b3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b7374-69b1-4f1a-9c36-eb2c222e49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = {\n",
    "    'test': DataLoader(dataset['test'], batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=custom_collate_fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a1ac2-f379-4e50-9481-f5118dcb6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/lora_crossattn_1e-5_ep1_model.pt', weights_only=False), strict=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a063b3-c634-49f4-95c9-92b5f10bf3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for data in tqdm(dataloader['test']):\n",
    "    for data_id, question_type, image, question in zip(*data):\n",
    "        # 1. disable cross attention\n",
    "        for i in range(len(model.language_model.model.layers)):\n",
    "            model.language_model.model.layers[i].enable_cross_attn = False\n",
    "        \n",
    "        question = question_template.format(question)\n",
    "        question_inputs = processor(images=image, text=question, return_tensors='pt').to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            outputs = model(**question_inputs, use_cache=True)\n",
    "\n",
    "        # 2. enable cross attention\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            features = extractor.process_images([image])\n",
    "        for i in range(len(model.language_model.model.layers)):\n",
    "            model.language_model.model.layers[i].enable_cross_attn = True\n",
    "            model.language_model.model.layers[i].cross_attn_context_1 = features['patch_tokens']\n",
    "            model.language_model.model.layers[i].cross_attn_context_2 = features['instance_tokens']\n",
    "            model.language_model.model.layers[i].cross_attn_mask_1 = None\n",
    "            model.language_model.model.layers[i].cross_attn_mask_2 = features['instance_attention_mask']\n",
    "\n",
    "        answer = 'ASSISTANT:'\n",
    "        answer_inputs = processor(text=answer, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "        answer_embeds = model.get_input_embeddings()(answer_inputs['input_ids'])\n",
    "        attention_mask = torch.cat([question_inputs['attention_mask'], answer_inputs['attention_mask']], dim=1)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.autocast(device), torch.no_grad():\n",
    "            outputs = model(inputs_embeds=answer_embeds, attention_mask=attention_mask, past_key_values=outputs['past_key_values'], use_cache=True, num_logits_to_keep=1)\n",
    "\n",
    "        # 3. generate\n",
    "        response = []\n",
    "        for _ in range(MAX_TOKENS):\n",
    "            next_token_id = outputs['logits'].argmax(2)\n",
    "            response.append(next_token_id.item())\n",
    "            if next_token_id.item() == processor.tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=device)], dim=1)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.autocast(device), torch.no_grad():\n",
    "                outputs = model(input_ids=next_token_id, attention_mask=attention_mask, past_key_values=outputs['past_key_values'], use_cache=True)\n",
    "\n",
    "        generated_answer = processor.decode(response, skip_special_tokens=True)\n",
    "        predictions[data_id] = generated_answer\n",
    "        print(repr(data_id))\n",
    "        print(repr(generated_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4ef-00bb-410b-a1f0-b5eb83ac573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
